{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"porbabilistic_generative_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPtQSIbETmsMQ09Jg2/K8bE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uApglfuKtSRO"},"source":["## Porbabilistic generative model\n","\n","In this section we will discuss a generative approach to binary classification. Again, we will not go through the formulation detailedly. Please find [Prof. Lee's lecture](https://www.youtube.com/watch?v=fZAZUYEeIMg) if you are interested in it.\n","\n","接者我們將實作基於 generative model 的二元分類器，理論細節請參考[李宏毅老師的教學影片](https://www.youtube.com/watch?v=fZAZUYEeIMg)。\n","\n","### Preparing Data\n","\n","Training and testing data is loaded and normalized as in logistic regression. However, since LDA is a deterministic algorithm, there is no need to build a development set.\n","\n","訓練集與測試集的處理方法跟 logistic regression 一模一樣，然而因為 generative model 有可解析的最佳解，因此不必使用到 development set。"]},{"cell_type":"code","metadata":{"id":"eZK20N0DtRLy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635914510396,"user_tz":420,"elapsed":5438,"user":{"displayName":"陈孟君","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16666597963024522209"}},"outputId":"7f76ce3a-dcce-42a3-c756-e3449b309506"},"source":["!gdown --id '1KSFIRh0-_Vr7SdiSCZP1ItV7bXPxMD92' --output data.tar.gz\n","!tar -zxvf data.tar.gz\n","!ls"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1KSFIRh0-_Vr7SdiSCZP1ItV7bXPxMD92\n","To: /content/data.tar.gz\n","\r  0% 0.00/6.11M [00:00<?, ?B/s]\r100% 6.11M/6.11M [00:00<00:00, 95.0MB/s]\n","data/\n","data/sample_submission.csv\n","data/test_no_label.csv\n","data/train.csv\n","data/X_test\n","data/X_train\n","data/Y_train\n","data  data.tar.gz  sample_data\n"]}]},{"cell_type":"code","metadata":{"id":"-h4KEXf8G080","executionInfo":{"status":"ok","timestamp":1635915814002,"user_tz":420,"elapsed":868,"user":{"displayName":"陈孟君","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16666597963024522209"}}},"source":["import numpy as np\n","\n","np.random.seed(0)\n","X_train_fpath = './data/X_train'\n","Y_train_fpath = './data/Y_train'\n","X_test_fpath = './data/X_test'\n","output_fpath = './output_{}.csv'"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"RVXrVNyPG5rR","executionInfo":{"status":"ok","timestamp":1635915815639,"user_tz":420,"elapsed":4,"user":{"displayName":"陈孟君","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16666597963024522209"}}},"source":["def _normalize(X, train = True, specified_column = None, X_mean = None, X_std = None):\n","    # This function normalizes specific columns of X.\n","    # The mean and standard variance of training data will be reused when processing testing data.\n","    #\n","    # Arguments:\n","    #     X: data to be processed\n","    #     train: 'True' when processing training data, 'False' for testing data\n","    #     specific_column: indexes of the columns that will be normalized. If 'None', all columns\n","    #         will be normalized.\n","    #     X_mean: mean value of training data, used when train = 'False'\n","    #     X_std: standard deviation of training data, used when train = 'False'\n","    # Outputs:\n","    #     X: normalized data\n","    #     X_mean: computed mean value of training data\n","    #     X_std: computed standard deviation of training data\n","\n","    if specified_column == None:\n","        specified_column = np.arange(X.shape[1])\n","    if train:\n","        X_mean = np.mean(X[:, specified_column] ,0).reshape(1, -1)\n","        X_std  = np.std(X[:, specified_column], 0).reshape(1, -1)\n","\n","    X[:,specified_column] = (X[:, specified_column] - X_mean) / (X_std + 1e-8)\n","     \n","    return X, X_mean, X_std"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"SI3iDQHZGkHo","executionInfo":{"status":"ok","timestamp":1635915826187,"user_tz":420,"elapsed":8709,"user":{"displayName":"陈孟君","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16666597963024522209"}}},"source":["# Parse csv files to numpy array\n","with open(X_train_fpath) as f:\n","    next(f)\n","    X_train = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n","with open(Y_train_fpath) as f:\n","    next(f)\n","    Y_train = np.array([line.strip('\\n').split(',')[1] for line in f], dtype = float)\n","with open(X_test_fpath) as f:\n","    next(f)\n","    X_test = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n","\n","# Normalize training and testing data\n","X_train, X_mean, X_std = _normalize(X_train, train = True)\n","X_test, _, _= _normalize(X_test, train = False, specified_column = None, X_mean = X_mean, X_std = X_std)\n"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"_-wmH0FYI9u0","executionInfo":{"status":"ok","timestamp":1635915130596,"user_tz":420,"elapsed":338,"user":{"displayName":"陈孟君","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16666597963024522209"}}},"source":["train_size = X_train.shape[0]\n","test_size = X_test.shape[0]\n","data_dim = X_train.shape[1]"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"guFGCa2aHL6t"},"source":["#### Functions\n","Some functions that will be repeatedly used when iteratively updating the parameters.*斜體文字* "]},{"cell_type":"code","metadata":{"id":"Bg-yQtylHXQD","executionInfo":{"status":"ok","timestamp":1635914717777,"user_tz":420,"elapsed":412,"user":{"displayName":"陈孟君","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16666597963024522209"}}},"source":["def _predict(X, w, b):\n","    # This function returns a truth value prediction for each row of X \n","    # by rounding the result of logistic regression function.\n","    return np.round(_f(X, w, b)).astype(np.int)\n","    \n","def _accuracy(Y_pred, Y_label):\n","    # This function calculates prediction accuracy\n","    acc = 1 - np.mean(np.abs(Y_pred - Y_label))\n","    return acc"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ri_PIlUDH5r2"},"source":["#### Mean and Covariance"]},{"cell_type":"code","metadata":{"id":"_NaJq08GHt9X","executionInfo":{"status":"ok","timestamp":1635915199998,"user_tz":420,"elapsed":62376,"user":{"displayName":"陈孟君","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16666597963024522209"}}},"source":["# collect data\n","X_train_0 = np.array([x for x, y in zip(X_train, Y_train) if y == 0])\n","X_train_1 = np.array([x for x, y in zip(X_train, Y_train) if y == 1])\n","#Compute in-class mean\n","mean_0 = np.mean(X_train_0, axis = 0)\n","mean_1 = np.mean(X_train_1, axis = 0)  \n","\n","# create list\n","cov_0 = np.zeros((data_dim, data_dim))\n","cov_1 = np.zeros((data_dim, data_dim))\n","#Compute in-class covariance\n","for x in X_train_0:\n","    cov_0 += np.dot(np.transpose([x - mean_0]), [x - mean_0]) / X_train_0.shape[0]\n","for x in X_train_1:\n","    cov_1 += np.dot(np.transpose([x - mean_1]), [x - mean_1]) / X_train_1.shape[0]\n","\n","# Shared covariance is taken as a weighted average of individual in-class covariance.\n","cov = (cov_0 * X_train_0.shape[0] + cov_1 * X_train_1.shape[0]) / (X_train_0.shape[0] + X_train_1.shape[0])"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7x2IJHrBJACP","executionInfo":{"status":"ok","timestamp":1635915217811,"user_tz":420,"elapsed":425,"user":{"displayName":"陈孟君","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16666597963024522209"}},"outputId":"63ed752c-aead-46a7-929b-5ee99649f49e"},"source":["cov.shape"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(510, 510)"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"gH9NCUOMJ7xn"},"source":["#### Computing weights and bias"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KGunQSvbJSCZ","executionInfo":{"status":"ok","timestamp":1635915455677,"user_tz":420,"elapsed":305,"user":{"displayName":"陈孟君","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16666597963024522209"}},"outputId":"f1bf8ae0-0f5f-4745-d2af-1b997e9f8a2e"},"source":["# Compute inverse of covariance matrix.\n","# Since covariance matrix may be nearly singular, np.linalg.inv() may give a large numerical error.\n","# Via SVD decomposition, one can get matrix inverse efficiently and accurately.\n","u, s, v = np.linalg.svd(cov, full_matrices=False)\n","inv = np.matmul(v.T * 1 / s, u.T)\n","\n","# Directly compute weights and bias\n","w = np.dot(inv, mean_0 - mean_1)\n","b =  (-0.5) * np.dot(mean_0, np.dot(inv, mean_0)) + 0.5 * np.dot(mean_1, np.dot(inv, mean_1))\\\n","    + np.log(float(X_train_0.shape[0]) / X_train_1.shape[0]) \n","\n","# Compute accuracy on training set\n","Y_train_pred = 1 - _predict(X_train, w, b)\n","print('Training accuracy: {}'.format(_accuracy(Y_train_pred, Y_train)))"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Training accuracy: 0.8725486582129165\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_UPK8f3tKNmC","executionInfo":{"status":"ok","timestamp":1635915566902,"user_tz":420,"elapsed":438,"user":{"displayName":"陈孟君","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16666597963024522209"}},"outputId":"14ee48ab-c39d-4467-ee10-e15bf2f0cbd9"},"source":["predictions = 1 - _predict(X_test, w, b)\n","with open(output_fpath.format('generative_method'), 'w') as f:\n","    f.write('id,label\\n')\n","    for i, label in  enumerate(predictions):\n","        f.write('{},{}\\n'.format(i, label))\n","\n","# Print out the most significant weights\n","ind = np.argsort(np.abs(w))[::-1]\n","with open(X_test_fpath) as f:\n","    content = f.readline().strip('\\n').split(',')\n","features = np.array(content)\n","for i in ind[0:10]:\n","    print(features[i], w[i])"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":[" Retail trade 8.65380859375\n"," 34 -6.53564453125\n"," Utilities and sanitary services 6.458984375\n"," 37 -6.03125\n"," Other service -5.563720703125\n"," Tennessee -4.6640625\n"," Abroad 4.48828125\n"," Sales -4.28515625\n"," 17 -4.28515625\n"," Education 4.189453125\n"]}]},{"cell_type":"code","metadata":{"id":"cK2B2IilKowX"},"source":[""],"execution_count":null,"outputs":[]}]}